#!/usr/bin/perl

# s3 command - script for manipulating files on Amazon AWS S3 service

# Copyright (c) 2008, Oliver Crow
# See License.txt for terms of use

use Net::Amazon::S3;
use Text::Wrap;
use File::stat;
use File::Spec;
use Getopt::Std;
use strict;

my $VERSION = 0.5;

my (%config, $s3, $cmd);

sub usage_quit {
    my ($message) = @_;
    print "\n$message\n" unless ($message eq '');
    print "\nusage: $0 <command> [parameters]\n";
    print "commands:\n";
    print "   ls                            - list buckets\n";
    print "   ls   [-l] <bucket>[/<path>]   - list files\n";
    print "   find [-l] <bucket>[/<path>]   - list files recursively\n";
    print "   du   [-h] <bucket>[/<path>]   - show disk usage\n";
    print "   get  <bucket>[/<path>] <file> - get file from S3\n";
    print "   put  <bucket>[/<path>] <file> - put file to S3\n";
    print "   push <bucket>[/<path>] <file> "
        . "- put version to S3 if file has changed\n";
    print "   rm   <bucket>[/<path>] <file> - remove a file from S3\n";
    print "   info <bucket>[/<path>] <file> - display metadata for a file in S3\n";
    print "   diff <bucket>[/<path>] <file> "
        . "- compare local file with stored S3 checksum\n";
    print "   mkbucket <bucket>             - create a bucket\n";
    print "   rmbucket <bucket>             - remove an empty bucket\n";
    print "\n";
    exit(1);
}

sub argv_location {
    # get a bucket name, and optional path
    # and a file name from the command arguments, or die
    if (scalar(@ARGV) == 0) {
        error("usage: $0 $cmd <bucket>[/<path>]");
    }

    my $location = shift @ARGV;

    my ($bucket, $path) = split(/\//, $location, 2);

    if ($bucket eq '') {
        error("Bucket name cannot begin with '\/'")
            if ($location =~ /^\//);
        error("Please specify a bucket name");
    }

    # make sure path ends with /
    if ($path ne '' && substr($path, -1, 1) ne '/') {
        $path .= '/';
    }
    return ($bucket, $path);
}

sub argv_location_and_file {
    # get a bucket, path and file from argv
    # derive the S3 key from the path and file
    # return the bucket, key and file

    # get a bucket and file name from the command arguments, or die
    if (scalar(@ARGV) < 2) {
        error("usage: $0 $cmd <bucket>[/<path>] <file>");
    }

    my ($bucket, $path) = argv_location();

    # get the filename part from the file path
    my ($file) = shift @ARGV;
    my ($volume, $dir, $fname) = File::Spec->splitpath($file);
    my $key = $path . $fname;
    return ($bucket, $key, $file);
}

sub argv_bucket_name {
    # get a bucket name from the command arguments, or die
    if (scalar(@ARGV) == 0) {
        error("usage: $0 $cmd <bucket>");
    }
    my ($bucket) = split(/\//, shift @ARGV);
    return $bucket;
}

sub get_config {
    my $rcfile = "$ENV{'HOME'}/.s3rc";
    open RC, $rcfile
        or die "Couldn't open config file '$rcfile'";
    while(<RC>) {
        m/(\S+)\s*:\s*(\S+)/;
        $config{$1} = $2 if (defined $1);
    }
    close RC;
    if (!defined($config{'aws_access_key_id'})) {
        die "Config file didn't define aws_access_key_id";
    }
    if (!defined($config{'aws_secret_access_key'})) {
        die "Config file didn't define aws_secret_access_key";
    }
}

sub open_s3 {
    $s3 = Net::Amazon::S3->new({
        aws_access_key_id => $config{'aws_access_key_id'},
        aws_secret_access_key => $config{'aws_secret_access_key'},
        retry => 1
    });
}

sub s3_error {
    print "S3 Error: " . $s3->err . "\n";
    print wrap("", "", $s3->errstr) . "\n";
    exit(1);
}

sub error {
    my ($msg) = @_;
    print STDERR $msg . "\n";
    exit(1);
}

sub strip_prefix {
    my ($string, $prefix) = @_;
    if (index($string, $prefix) == 0) {
        $string = substr($string, length($prefix));
    }
    return $string;
}

sub list_files { 
    # list files in given location
    my %options;
    getopts('l', \%options);

    my ($bucket_name, $path) = argv_location();
    my %params = (
        'bucket' => $bucket_name,
        'prefix' => $path,
    );
    $params{'delimiter'} = '/' if ($cmd eq 'ls');

    my $response = $s3->list_bucket_all(\%params)
        or s3_error();

    exists $response->{bucket}
        or error("No such bucket '$bucket_name'");

    foreach my $dir (@{$response->{common_prefixes}}) {
        # strip off the path from the results
        $dir = strip_prefix($dir, $path);
        print "$dir/\n";
    }

    foreach my $key (@{$response->{keys}}) {
        my $name = $key->{key};

        # strip off the path from the results
        $name = strip_prefix($name, $path);
        if (defined($options{l})) {
            my $date = $key->{last_modified};
            $date =~ s/T/ /; $date =~ s/\.\d\d\dZ//;
            printf("%10d  %s  %s\n", 
                $key->{size}, $date, $name);
        } else {
            printf("%s\n", $name);
        }
    }
}

sub list_buckets {
    my $buckets = $s3->buckets
        or s3_error();
    foreach my $bucket (@{$buckets->{buckets}}) {
        print $bucket->bucket . "\n";
    }
}

sub show_list {
    if (scalar(@ARGV) > 0) {
        list_files();
    } else {
        list_buckets();
    }
}

sub human_size {
    # format a byte count in kilobytes, megabytes, etc.
    my ($count) = @_;
    my @suffixes = ('B', 'k', 'M', 'G', 'T', 'P');
    while ($count >= 1024) {
        $count /= 1024; 
        shift @suffixes;
    }
    if ($count >= 10  || $suffixes[0] eq 'B') {
        return sprintf "%d%s", $count, $suffixes[0];
    } else {
        return sprintf "%.1f%s", int($count * 10) / 10, $suffixes[0];
    }
}

sub show_usage {
    my %options;
    getopts('h', \%options);

    if (scalar(@ARGV) < 1) {
        error("usage: $0 du [-h] <bucket>[/<path>]");
    }

    my ($bucket_name, $path) = argv_location();
    my %params = (
        'bucket' => $bucket_name,
        'prefix' => $path,
    );
    my $response = $s3->list_bucket_all(\%params)
        or s3_error();

    exists $response->{bucket}
        or error("No such bucket '$bucket_name'");

    my $total = 0;
    foreach my $key (@{$response->{keys}}) {
        $total += $key->{size};
    }
    if (defined($options{'h'})) {
        print human_size($total) . "\n";
    } else {
        print "$total\n";
    }
}

sub get_file { 
    my ($bucket_name, $key, $file) = argv_location_and_file();

    my $bucket=$s3->bucket($bucket_name);
    my $meta_data = $bucket->head_key($key)
        or error("No such file '$key'");

    # don't clobber local file
    ! -e $file or error("File '$file' already exists");

    # get file contents
    $bucket->get_key_filename($key, "GET", $file)
        or error("No such file '$key'");

    my $remote_mtime = $meta_data->{'x-amz-meta-file-mtime'};
    if (defined($remote_mtime)) {
        utime time, $remote_mtime, $file;
    }
}

sub put_file {
    my ($bucket_name, $key, $file) = argv_location_and_file();
    send_file($bucket_name, $key, $file);
}

sub send_file { 
    my ($bucket_name, $key, $file) = @_;

    -f $file or error("No such file '$file'");
    -r $file or error("Can't read file '$file'");

    # get file modification date
    my $mtime = (stat $file)->mtime;

    my $bucket=$s3->bucket($bucket_name);

    if (-z $file) {
        # special case for empty files
        $bucket->add_key($key, "", 
            {'x-amz-meta-file-mtime' => $mtime})
            or s3_error();
    } else {
        $bucket->add_key_filename($key, $file, 
            {'x-amz-meta-file-mtime' => $mtime})
            or s3_error();
    }
}

sub latest_version_info {
    # retrieve file info of most recent available version of a file in s3

    my ($bucket_name, $key) = @_;
    my $response = $s3->list_bucket_all({
        'bucket' => $bucket_name,
        'prefix' => $key,
    }) or s3_error();

    exists $response->{bucket}
        or error("No such bucket '$bucket_name'");

    # find highest used version number
    
    my ($version, $max_version, $key_data);
    foreach my $k (@{$response->{keys}}) {
        my $suffix = strip_prefix($k->{key}, $key);
        if ($suffix eq '') {
            $version = 0;
        } elsif ($suffix =~ /^,(\d+)$/) {
            $version = $1;
        } else {
            next;
        }
        if ($version >= $max_version) {
            $max_version = $version;
            $key_data = $k;
        }
    }
    $key_data->{version} = $version if (defined($key_data)); 
    return $key_data;
}

sub version_key {
    # get version_key for a given version number and base key
    my ($key, $version) = @_;
    return ($version == 0) ? $key : "$key,$version";
}

sub push_file {
    # send file to S3 if it isn't already there 
    # if it has changed, as determined by file size and modification date 
    # send the new version (keeping the previous version(s) intact)

    my ($bucket_name, $key, $file) = argv_location_and_file();

    -f $file or error("File '$file' doesn't exist locally");
    -r $file or error("File '$file' isn't readable");

    my $file_info = latest_version_info($bucket_name, $key);

    # file doesn't exist on S3, so send it
    if (!defined($file_info)) {
        send_file($bucket_name, $key, $file);
        return;
    }
    
    my $version_key = $file_info->{key};
    my $bucket=$s3->bucket($bucket_name);
    my $meta_data = $bucket->head_key($version_key);

    # compare local and remote file sizes and modification times
    my $local_size = (-s $file);
    my $local_mtime = (stat $file)->mtime;

    my $remote_size = $meta_data->{content_length};
    my $remote_mtime = $meta_data->{'x-amz-meta-file-mtime'};

    if ($local_size != $remote_size || $local_mtime ne $remote_mtime) {
        my $new_version = $file_info->{version} + 1;
        my $new_version_key = version_key($key, $new_version);
        send_file($bucket_name, $new_version_key, $file);
        return;
    }
    exit(2);
}

sub remove_file { 
    my ($bucket_name, $key, $file) = argv_location_and_file();
    
    my $bucket=$s3->bucket($bucket_name);
    $bucket->delete_key($key)
        or s3_error();
} 

sub file_info {
    my ($bucket_name, $key, $file) = argv_location_and_file();
    my $bucket=$s3->bucket($bucket_name);
    my $meta_data = $bucket->head_key($key);

    if (!defined($meta_data)) { 
        error("File '$key' doesn't exist in $bucket_name"); 
    }
    foreach my $field (keys %$meta_data) {
        print "$field: $meta_data->{$field}\n" 
            unless ($field eq "value");
    }
}

sub md5_file {
    # get md5 checksum of local file
    my ($file) = @_;
    use Digest::MD5;

    open FILE, $file or error("Can't read file '$file': $!");
    binmode FILE;
    my $md5 = Digest::MD5->new;
    $md5->addfile(*FILE);
    return $md5->hexdigest;
}

sub diff_file {
    # compare local file with version on S3 using file sizes and MD5 checksums
    # to determine if the files differ

    my ($bucket_name, $key, $file) = argv_location_and_file();

    -f $file or error("File '$file' doesn't exist locally");
    -r $file or error("File '$file' isn't readable");

    my $file_info = latest_version_info($bucket_name, $key);

    defined($file_info) or error("File '$key' doesn't exist on S3"); 

    # compare MD5 checksums
    my $remote_md5 = $file_info->{etag};
    my $local_md5 = md5_file($file);

    if ($local_md5 ne $remote_md5) {
        printf STDERR "Checksum differs %s\n"
            . "local:  %s\n"
            . "remote: %s\n",
            $file_info->{key}, $local_md5, $remote_md5;
        exit(2);
    }
    printf "Identical $file_info->{key}\n";
}

sub make_bucket {
    my $bucket_name = argv_bucket_name();
    $s3->add_bucket( { bucket => $bucket_name } ) or s3_error();
}

sub remove_bucket { 
    my $bucket_name = argv_bucket_name();
    my $bucket=$s3->bucket($bucket_name);
    $bucket->delete_bucket or s3_error();
}

my %commands = (
    'ls'   => \&show_list,
    'find' => \&show_list,
    'du'   => \&show_usage,
    'info' => \&file_info,
    'get'  => \&get_file,
    'put'  => \&put_file,
    'push' => \&push_file,
    'rm'   => \&remove_file,
    'diff' => \&diff_file,
    'mkbucket' => \&make_bucket,
    'rmbucket' => \&remove_bucket,
);

$cmd = shift @ARGV;
if (!defined($cmd)) {
    usage_quit();
}
get_config();
open_s3();

if (defined($commands{$cmd})) {
    $commands{$cmd}->();
} else {
    print STDERR "Unknown command '$cmd'\n";
    exit(1);
}


=head1 NAME

s3 - tool for manipulating files stored in Amazon S3

=head1 SYNOPSIS

 s3 ls 
 s3 ls   [-l] <bucket>[/<path>]
 s3 find [-l] <bucket>[/<path>]
 s3 du   [-h] <bucket>[/<path>]
 s3 get  <bucket>[/<path>] <file>
 s3 put  <bucket>[/<path>] <file>
 s3 push <bucket>[/<path>] <file>
 s3 rm   <bucket>[/<path>] <file>
 s3 info <bucket>[/<path>] <file>
 s3 diff <bucket>[/<path>] <file>
 s3 mkbucket <bucket> 
 s3 rmbucket <bucket>

=head1 DESCRIPTION

B<s3> provides a command line interface to Amazon 
Simple Storage Service.  You can list files on S3, get and 
retrieve files, delete files, create and delete buckets.  B<s3> uses
the Net::Amazon::S3 perl module to communicate with the Amazon S3 service.

=head2 Configuration

Two pieces of identifying information are required to access Amazon S3 - your
Access Key ID and your Secret Access Key.  After signing up for
Amazon Web Services find these on the AWS Access Identifiers web page.
Put the two keys into the ~/.s3rc file.  Use the following format, 
replacing Xs with your actual keys:

 aws_access_key_id: XXXXXXXXXXXXXXXXXXXX
 aws_secret_access_key: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

=head2 Buckets

Files are stored on Amazon S3 in buckets.  You must create at least one bucket in
which to store your files.  The set of available bucket names is shared 
with all other Amazon S3 users.  Choosing a bucket name that includes
a domain name that you control may help in finding a bucket name that 
isn't already in use.

=over

=item B<s3 mkbucket> <bucket>

Create a new bucket in S3.  If the bucket already exists an error is reported
only if the bucket is owned by someone else and you don't have access to it.

=item B<s3 rmbucket> <bucket>

Delete an empty bucket from S3.  An error is reported if the bucket isn't empty
or if you don't have access to it.

=item B<s3 ls>

Display a list of the buckets to which you have access.

=head2 Listing files

B<s3> organizes files within each bucket in a hierarchy, similar to a
filesystem directory structure.  The "/" slash character is used to separate
the bucket name from the path and to divide the path into 
directory components.  Directories in S3 are not explicitly 
created or deleted.  A directory exists when it contains at least one file, 
and disappears once the final file in that directory is deleted.

=item B<s3 ls> [-l] <bucket>[/<path>]

Display a list of S3 directories and files.  Directories are listed first,
each with a trailing slash.  Files are listed next, in alphabetical order. 
If the C<-l> flag is used a long listing is generated showing the size
in bytes and the modification time of each file.  Only files and
directories at the specified path are listed, or top-level files and 
directories if no path is given.

=item B<s3 find> [-l] <bucket>[/<path>]

Display list of all files beneath the specified path at any directory depth.
This is similar to ls, except it descends all directories recursively.

=item B<s3 du> [-h] <bucket>[/<path>]

Show total number of bytes used by all files within the given bucket and path,
at any directory depth. 
If the -h option is given the total is formatted into a human readable format,
using a one letter Byte, Kilobyte, Megabyte, Gigabyte, Terabyte or Petabyte 
suffix. Each unit is 1024 times bigger than the last.

=head2 Manipulating files

B<s3> assumes that files will have the same filename in Amazon S3 as they
do on the local file system.  B<s3> does not require the local directory
structure to match the path structure in Amazon S3. 

Commands to manipulate files take an S3 location parameter followed by
a local file parameter.  The location parameter consists of a bucket 
name followed by an optional path within that bucket.  The path does not include
the file name.  The file 
parameter specifies the path to the file on the local filesystem, including
the file name.  The name of the file on S3 is inferred to be the same 
as the local file name (i.e. the last element of the file path).

=item B<s3 get> <bucket>[/<path>] <file>

Get a file from S3.  An error is reported if the file already exists locally.

The modification date of the local file is reset to that of the original
local file.  This ensures that putting a file and then getting it doesn't
affect the file modification date.

=item B<s3 put> <bucket>[/<path>] <file>

Put a file to S3.  An error is reported if the local file can't be read.
If the file already exists in S3 it is overwritten. 

=item B<s3 push> <bucket>[/<path>] <file>

Put a file to S3 unless it's already there and without overwriting it.
The file is not sent if it already exists in S3 and has the same
size and modification date as the the local file.

If the local file has changed, a new copy is saved to S3 with a version
number appended to the file name. The version number is separated 
from the rest of the file name by a comma.  If several versions of a 
file already exist in S3, the most recent is used to determine whether 
the local file has changed.

=item B<s3 rm> <bucket>[/<path>] <file>

Delete a file from S3.

=item B<s3 info> <bucket>[/<path>] <file>

Display a file's S3 meta-data. An error is reported if the file doesn't
exist in S3.

=item B<s3 diff> <bucket>[/<path>] <file>

Compare the MD5 checksums of the local file and S3 file.  
If there are several versions of the file in S3, the most recent is used
(see B<s3 push> for details).
If the local or remote files don't exist or aren't readable an error is 
reported and the exit status is 1. 
If the files differ an error is reported and the exit status is 2.
This command computes the local file checksum, and relies on the S3 checksum
computed by Amazon when the file was saved in S3.

=head1 BUGS

Amazon S3 cannot store individual files of size greater than 5GB. 
This program does not attempt to overcome that limitation.

=head1 FILES

~/.pinerc    Per-user configuration file for S3 access keys

=head1 AUTHOR

Oliver Crow (ocrow@simplexity.net)

=head1 COPYRIGHT

 Copyright (c) 2008, Oliver Crow
 Distributed under OSI MIT License

=cut


