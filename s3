#!/usr/bin/perl

# s3 command - script for manipulating files on Amazon AWS S3 service

# Copyright (c) 2008, Oliver Crow
# See License.txt for terms of use

use Net::Amazon::S3;
use Text::Wrap;
use File::stat;
use File::Spec;
use Getopt::Std;
use strict;

my $VERSION = 0.5;

my (%config, $s3, $cmd);

sub usage_quit {
    my ($message) = @_;
    print "\n$message\n" unless ($message eq '');
    print "\nusage: $0 <command> [parameters]\n";
    print "commands:\n";
    print "   ls                            - list buckets\n";
    print "   ls   [-l] <bucket>[/<path>]   - list files\n";
    print "   find [-l] <bucket>[/<path>]   - list files recursively\n";
    print "   get  <bucket>[/<path>] <file> - get file from S3\n";
    print "   put  <bucket>[/<path>] <file> - put file to S3\n";
    print "   push <bucket>[/<path>] <file> "
        . "- put version to S3 if file has changed\n";
    print "   rm   <bucket>[/<path>] <file> - remove a file from S3\n";
    print "   info <bucket>[/<path>] <file> - display metadata for a file in S3\n";
    print "   diff <bucket>[/<path>] <file> "
        . "- compare local file with stored S3 checksum\n";
    print "   mkbucket <bucket>             - create a bucket\n";
    print "   rmbucket <bucket>             - remove an empty bucket\n";
    print "\n";
    exit(1);
}

sub argv_location {
    # get a bucket name, and optional path
    # and a file name from the command arguments, or die
    if (scalar(@ARGV) == 0) {
        error("usage: $0 $cmd <bucket>[/<path>]");
    }

    my $location = shift @ARGV;

    my ($bucket, $path) = split(/\//, $location, 2);

    if ($bucket eq '') {
        error("Bucket name cannot begin with '\/'")
            if ($location =~ /^\//);
        error("Please specify a bucket name");
    }

    # make sure path ends with /
    if ($path ne '' && substr($path, -1, 1) ne '/') {
        $path .= '/';
    }
    return ($bucket, $path);
}

sub argv_location_and_file {
    # get a bucket, path and file from argv
    # derive the S3 key from the path and file
    # return the bucket, key and file

    # get a bucket and file name from the command arguments, or die
    if (scalar(@ARGV) < 2) {
        error("usage: $0 $cmd <bucket>[/<path>] <file>");
    }

    my ($bucket, $path) = argv_location();

    # get the filename part from the file path
    my ($file) = shift @ARGV;
    my ($volume, $dir, $fname) = File::Spec->splitpath($file);
    my $key = $path . $fname;
    return ($bucket, $key, $file);
}

sub argv_bucket_name {
    # get a bucket name from the command arguments, or die
    if (scalar(@ARGV) == 0) {
        error("usage: $0 $cmd <bucket>");
    }
    my ($bucket) = split(/\//, shift @ARGV);
    return $bucket;
}

sub get_config {
    my $rcfile = "$ENV{'HOME'}/.s3rc";
    open RC, $rcfile
        or die "Couldn't open config file '$rcfile'";
    while(<RC>) {
        m/(\S+)\s*:\s*(\S+)/;
        $config{$1} = $2 if (defined $1);
    }
    close RC;
    if (!defined($config{'aws_access_key_id'})) {
        die "Config file didn't define aws_access_key_id";
    }
    if (!defined($config{'aws_secret_access_key'})) {
        die "Config file didn't define aws_secret_access_key";
    }
}

sub open_s3 {
    $s3 = Net::Amazon::S3->new({
        aws_access_key_id => $config{'aws_access_key_id'},
        aws_secret_access_key => $config{'aws_secret_access_key'},
        retry => 1
    });
}

sub s3_error {
    print "S3 Error: " . $s3->err . "\n";
    print wrap("", "", $s3->errstr) . "\n";
    exit(1);
}

sub error {
    my ($msg) = @_;
    print STDERR $msg . "\n";
    exit(1);
}

sub strip_prefix {
    my ($string, $prefix) = @_;
    if (index($string, $prefix) == 0) {
        $string = substr($string, length($prefix));
    }
    return $string;
}

sub list_files { 
    # list files in given location
    my %options;
    getopts('l', \%options);

    my ($bucket_name, $path) = argv_location();
    my %params = (
        'bucket' => $bucket_name,
        'prefix' => $path,
    );
    $params{'delimiter'} = '/' if ($cmd eq 'ls');

    my $response = $s3->list_bucket_all(\%params)
        or s3_error();

    exists $response->{bucket}
        or error("No such bucket '$bucket_name'");

    foreach my $dir (@{$response->{common_prefixes}}) {
        # strip off the path from the results
        $dir = strip_prefix($dir, $path);
        print "$dir/\n";
    }

    foreach my $key (@{$response->{keys}}) {
        my $name = $key->{key};

        # strip off the path from the results
        $name = strip_prefix($name, $path);
        if (defined($options{l})) {
            my $date = $key->{last_modified};
            $date =~ s/T/ /; $date =~ s/\.\d\d\dZ//;
            printf("%10d  %s  %s\n", 
                $key->{size}, $date, $name);
        } else {
            printf("%s\n", $name);
        }
    }
}

sub list_buckets {
    my $buckets = $s3->buckets
        or s3_error();
    foreach my $bucket (@{$buckets->{buckets}}) {
        print $bucket->bucket . "\n";
    }
}

sub show_list {
    if (scalar(@ARGV) > 0) {
        list_files();
    } else {
        list_buckets();
    }
}

sub get_file { 
    my ($bucket_name, $key, $file) = argv_location_and_file();

    my $bucket=$s3->bucket($bucket_name);
    my $meta_data = $bucket->head_key($key)
        or error("No such file '$key'");

    # don't clobber local file
    ! -e $file or error("File '$file' already exists");

    # get file contents
    $bucket->get_key_filename($key, "GET", $file)
        or error("No such file '$key'");

    my $remote_mtime = $meta_data->{'x-amz-meta-file-mtime'};
    if (defined($remote_mtime)) {
        utime time, $remote_mtime, $file;
    }
}

sub put_file {
    my ($bucket_name, $key, $file) = argv_location_and_file();
    send_file($bucket_name, $key, $file);
}

sub send_file { 
    my ($bucket_name, $key, $file) = @_;

    -f $file or error("No such file '$file'");
    -r $file or error("Can't read file '$file'");

    # get file modification date
    my $mtime = (stat $file)->mtime;

    my $bucket=$s3->bucket($bucket_name);

    if (-z $file) {
        # special case for empty files
        $bucket->add_key($key, "", 
            {'x-amz-meta-file-mtime' => $mtime})
            or s3_error();
    } else {
        $bucket->add_key_filename($key, $file, 
            {'x-amz-meta-file-mtime' => $mtime})
            or s3_error();
    }
}

sub latest_version_info {
    # retrieve file info of most recent available version of a file in s3

    my ($bucket_name, $key) = @_;
    my $response = $s3->list_bucket_all({
        'bucket' => $bucket_name,
        'prefix' => $key,
    }) or s3_error();

    exists $response->{bucket}
        or error("No such bucket '$bucket_name'");

    # find highest used version number
    
    my ($version, $max_version, $key_data);
    foreach my $k (@{$response->{keys}}) {
        my $suffix = strip_prefix($k->{key}, $key);
        if ($suffix eq '') {
            $version = 0;
        } elsif ($suffix =~ /^,(\d+)$/) {
            $version = $1;
        } else {
            next;
        }
        if ($version > $max_version) {
            $max_version = $version;
            $key_data = $k;
        }
    }
    $key_data->{version} = $version if (defined($key_data)); 
    return $key_data;
}

sub version_key {
    # get version_key for a given version number and base key
    my ($key, $version) = @_;
    return ($version == 0) ? $key : "$key,$version";
}

sub push_file {
    # send file to S3 if it isn't already there 
    # if it has changed, as determined by file size and modification date 
    # send the new version (keeping the previous version(s) intact)

    my ($bucket_name, $key, $file) = argv_location_and_file();

    -f $file or error("File '$file' doesn't exist locally");
    -r $file or error("File '$file' isn't readable");

    my $file_info = latest_version_info($bucket_name, $key);

    # file doesn't exist on S3, so send it
    if (!defined($file_info)) {
        send_file($bucket_name, $key, $file);
        return;
    }
    
    my $version_key = $file_info->{key};
    my $bucket=$s3->bucket($bucket_name);
    my $meta_data = $bucket->head_key($version_key);

    # compare local and remote file sizes and modification times
    my $local_size = (-s $file);
    my $local_mtime = (stat $file)->mtime;

    my $remote_size = $meta_data->{content_length};
    my $remote_mtime = $meta_data->{'x-amz-meta-file-mtime'};

    if ($local_size != $remote_size || $local_mtime ne $remote_mtime) {
        my $new_version = $file_info->{version} + 1;
        my $new_version_key = version_key($key, $new_version);
        send_file($bucket_name, $new_version_key, $file);
        return;
    }
    exit(2);
}

sub remove_file { 
    my ($bucket_name, $key, $file) = argv_location_and_file();
    
    my $bucket=$s3->bucket($bucket_name);
    $bucket->delete_key($key)
        or s3_error();
} 

sub file_info {
    my ($bucket_name, $key, $file) = argv_location_and_file();
    my $bucket=$s3->bucket($bucket_name);
    my $meta_data = $bucket->head_key($key);

    if (!defined($meta_data)) { 
        error("File '$key' doesn't exist in $bucket_name"); 
    }
    foreach my $field (keys %$meta_data) {
        print "$field: $meta_data->{$field}\n" 
            unless ($field eq "value");
    }
}

sub diff_file {
    # compare local file with version on S3 using file sizes and MD5 checksums
    # to determine if the files differ
    use Digest::MD5;
    my ($bucket_name, $key, $file) = argv_location_and_file();
    
    -f $file or error("File '$file' doesn't exist locally");
    -r $file or error("File '$file' isn't readable");

    my $bucket=$s3->bucket($bucket_name);
    my $meta_data = $bucket->head_key($key);

    defined($meta_data) or error("File '$key' doesn't exist on S3"); 

    # compare file sizes
    my $local_size = (-s $file);
    my $remote_size = $meta_data->{content_length};

    if ($local_size != $remote_size) {
        printf STDERR "Files differ in size\nlocal:  %10d\nremote: %10d\n", 
            $local_size, $remote_size;
        exit(2);
    }

    # compare MD5 checksums
    my $remote_md5 = $meta_data->{etag};
    open FILE, $file or error("Can't read file '$file': $!");
    binmode FILE;
    my $md5 = Digest::MD5->new;
    $md5->addfile(*FILE);
    my $local_md5 = $md5->hexdigest;

    if ($local_md5 ne $remote_md5) {
        printf STDERR "File checksums differ\nlocal:  %s\nremote: %s\n",
            $local_md5, $remote_md5;
        exit(2);
    }
}

sub make_bucket {
    my $bucket_name = argv_bucket_name();
    $s3->add_bucket( { bucket => $bucket_name } ) or s3_error();
}

sub remove_bucket { 
    my $bucket_name = argv_bucket_name();
    my $bucket=$s3->bucket($bucket_name);
    $bucket->delete_bucket or s3_error();
}

my %commands = (
    'ls'   => \&show_list,
    'find' => \&show_list,
    'info' => \&file_info,
    'get'  => \&get_file,
    'put'  => \&put_file,
    'push' => \&push_file,
    'rm'   => \&remove_file,
    'diff' => \&diff_file,
    'mkbucket' => \&make_bucket,
    'rmbucket' => \&remove_bucket,
);

$cmd = shift @ARGV;
if (!defined($cmd)) {
    usage_quit();
}
get_config();
open_s3();

if (defined($commands{$cmd})) {
    $commands{$cmd}->();
} else {
    print STDERR "Unknown command '$cmd'\n";
    exit(1);
}


=head1 NAME

s3 - tool for manipulating files stored in Amazon S3

=head1 SYNOPSIS

 s3 ls 
 s3 ls   [-l] <bucket>[/<path>]
 s3 find [-l] <bucket>[/<path>]
 s3 get  <bucket>[/<path>] <file>
 s3 put  <bucket>[/<path>] <file>
 s3 push <bucket>[/<path>] <file>
 s3 rm   <bucket>[/<path>] <file>
 s3 info <bucket>[/<path>] <file>
 s3 diff <bucket>[/<path>] <file>
 s3 mkbucket <bucket> 
 s3 rmbucket <bucket>

=head1 DESCRIPTION

B<s3> provides a command line interface to Amazon 
Simple Storage Service.  You can list files on S3, get and 
retrieve files, delete files, create and delete buckets.  B<s3> uses
the Net::Amazon::S3 perl module to communicate with the Amazon S3 service.

=head2 Configuration

Two pieces of identifying information are required to access Amazon S3 - your
Access Key ID and your Secret Access Key.  After signing up for
Amazon Web Services find these on the AWS Access Identifiers web page.
Put the two keys into the ~/.s3rc file.  Use the following format, 
replacing Xs with your actual keys:

 aws_access_key_id: XXXXXXXXXXXXXXXXXXXX
 aws_secret_access_key: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

=head2 Buckets

Files are stored on Amazon S3 in buckets.  You must create at least one bucket in
which to store your files.  The set of available bucket names is shared 
with all other Amazon S3 users.  Choosing a bucket name that includes
a domain name that you control may help in finding a bucket name that 
isn't already in use.

=over

=item s3 mkbucket <bucket>

Create a new bucket in S3.  If the bucket already exists an error is reported
only if the bucket is owned by someone else and you don't have access to it.

=item s3 rmbucket <bucket>

Delete an empty bucket from S3.  An error is reported if the bucket isn't empty
or if you don't have access to it.

=item s3 ls

Display a list of the buckets to which you have access.

=head2 Listing files

B<s3> organizes files within each bucket in a hierarchy, similar to a
filesystem directory structure.  The "/" slash character is used to separate
the bucket name from the path and to divide the path into 
directory components.  Directories in S3 are not explicitly 
created or deleted.  A directory exists when it contains at least one file, 
and disappears once the final file in that directory is deleted.

=item s3 ls [-l] <bucket>[/<path>]

Display a list of S3 directories and files.  Directories are listed first,
each with a trailing slash.  Files are listed next, in alphabetical order. 
If the C<-l> flag is used a long listing is generated showing the size
in bytes and the modification time of each file.  Only files and
directories at the specified path are listed, or top-level files and 
directories if no path is given.

=item s3 find [-l] <bucket>[/<path>]

Display list of all files beneath the specified path at any directory depth.
This is similar to ls, except it descends all directories recursively.

=head2 Manipulating files

B<s3> assumes that files will have the same filename in Amazon S3 as they
do on the local file system.  B<s3> does not require the local directory
structure to match the path structure in Amazon S3. 

Commands to manipulate files take an S3 location parameter followed by
a local file parameter.  The location parameter consists of a bucket 
name followed by an optional path within that bucket.  The path does not include
the file name.  The file 
parameter specifies the path to the file on the local filesystem, including
the file name.  The name of the file on S3 is inferred to be the same 
as the local file name (i.e. the last element of the file path).

=item s3 get <bucket>[/<path>] <file>

Get a file from S3.  An error is reported if the file already exists locally.

The modification date of the local file is reset to that of the original
local file.  This ensures that putting a file and then getting it doesn't
affect the file modification date.

=item s3 put <bucket>[/<path>] <file>

Put a file to S3.  An error is reported if the local file can't be read.
If the file already exists in S3 it is overwritten. 

=item s3 push <bucket>[/<path>] <file>

Put a file to S3 only if necessary.  
If the file already exists in S3 and has the same size and modification 
date as the local file, no action is taken.  

If the local file has changed since the S3 copy was made, as judged 
by the file size and modification date, a new copy is sent to S3.
Rather than overwriting the original, a file version number is appended 
to the file name. File version numbers are separated from the rest of 
the file name by a comma, 
and begin at one for the first copy after the original.

When checking if the file has been modified, the most recent available 
version in S3 is used.

=item s3 rm <bucket>[/<path>] <file>


=item s3 info <bucket>[/<path>] <file>


=item s3 diff <bucket>[/<path>] <file>



=head1 FILES

~/.pinerc    Per-user configuration file for S3 access keys
    

=cut


